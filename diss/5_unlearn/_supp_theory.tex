\section{Theoretical Results}
\subsection{Proof of Lemma 1}
Let us take $D$ to be the training set; w.l.o.g. $z$ is the point being removed. Let the residual dataset be $D' = D \setminus {z}$.
Denote $w^-_{Full}$ as the weight parameters after doing a full Hessian update and $w^-_{Foci}$ as the weight parameters after doing a FOCI selected Hessian update.
In an ideal case, we want $(w^-_{Foci}, D')$/$(w^-_{Full}, D')$ to be as close as possible to $(w^*, D')$. Note that we consider both $(w^*, D)$ and $(w^*, D')$ to be $0$ as we don't expect model parameters to change drastically for one sample once trained to convergence. 

\begin{lemma}
The gap between the gradient residual norm of the FOCI Unlearning update in Algorithm~\ref{alg:blockunlearn} 1 and a full unlearning update via Eq.~\eqref{eq:sekhariunlearn} in the main thesis,
\begin{align}
||\nabla \mathcal{L}(w^-_{Foci},D')||_2 - ||\nabla \mathcal{L}(w^-_{Full},D')||_2
\end{align}
shrinks as $O(1/n^2)$.
\end{lemma}
\begin{proof}
Let $w$ to be a network of many linear layers with possible activation functions; we can think of the norm as the sum of norm of gradients for each layer. Hence, for any model parameters $w$ and dataset $D$, we have:
% strong assumption here, tbd add details
\begin{align}
    \label{eq:res_norm}
    ||\nabla \mathcal{L}(w,D)||_2 \coloneqq \sum_{l \in L} ||\nabla \mathcal{L}(w_l, D) ||_2
\end{align}

FOCI identifies a subset $T \subset L$ slices or layers that are to be updated. 
Let $R = L \setminus T$ be the remainder of the network which is not updated.
Hence, \ref{eq:res_norm} for $(w^-_{Foci}, D')$ can be written as:
\begin{align}
    \label{eq:foci_res_norm}
    & ||\nabla \mathcal{L}(w^-_{Foci},D')||_2 \coloneqq \sum_{l \in L} ||\nabla \mathcal{L}(w^-_{Foci_l}, D') ||_2 \\
    & = \sum_{l \in T} ||\nabla \mathcal{L}(w^-_{Foci_l}, D') ||_2 + 
     \sum_{l \in R} ||\nabla \mathcal{L}(w^-_{Foci_l}, D') ||_2 \\
    & = \sum_{l \in T} ||\nabla \mathcal{L}(w^-_{Foci_l}, D') ||_2 + 
    \sum_{l \in R} ||\nabla \mathcal{L}(w^*_{l}, D') ||_2
\end{align}
The last line follows from the fact that layers in $R$ are not updated.

We will next show how for the remainder of the dataset $D'$, the changes in $T$ propagate minimally when there are a large number of data points, $n$ in the training set.

W.L.O.G. assume that we have a $3$ layer network with the form:
\begin{align}
    (L_3(L_2(L_1(x)))
\end{align}
For the point being removed $z := (x,y)$; let $L_2$ be the intermediate layer which is selected for update by FOCI.
Before the update, activations out of $L_2$ are of the form $a_2 = L_2(L_1(x)) = L_2(a_1)$.
After the update, activations out of $L_2$ can be written as:
\begin{align}
    a_2' = L_2'(L_1(x)) &= L_2'(a_1) \\
    & = w_2' a_1 \\
    & = (w_2 + \delta_{w_2}) a_1 \\
    & = w_2 a_1 + \delta_{w_2} a_1 \\
    & = a_2 + \delta_{w_2} a_1
\end{align}
The Second line follows because $L_1$ isn't updated.
For the following layer $L_3$, we have $a_3 = L_3(a_2)$ before the update. After,
\begin{align}
    a_3' &= L_3(a_2') \\
    &= L_3(a_2 + \delta_{w_2} a_1) \\
    &= L_3(a_2) + \nabla L_3 (a_2) \delta_{w_2} a_1 + \mathcal{O}((\delta_{w_2} a_1)^2) \\
    &= L_3(a_2) + 0 + \mathcal{O}((\delta_{w_2} a_1)^2) 
\end{align}
The first-order term goes to zero, as $L_3$ has not been updated and we assume full model convergence.

For the \cite{sekhari2021remember} update.
\begin{align}
    \delta_{w_2} = \frac{1}{(n-1)}(\hat{H}^{-1})\sum_{z \in \{(x_k, y_k)\}} \nabla f(\hat{w}, z)
\end{align}
Hence, $\delta_{w_2}^2 \propto \frac{1}{n^2}$. Therefore, for large values of $n$, the third term in the equation above approaches $0$. So, $a_3' = L_3(a_2)$. This shows that propagation is minimal. Similar arguments regarding null space for over-parameterized deep networks have been mentioned in \cite{golatkar2020forgetting}. 

Now, looking back at the residual gradient norm, we have:
\begin{align}
    \label{eq:foci_res_norm_simplified}
    ||\nabla \mathcal{L}(w^-_{Foci},D')||_2 &= \sum_{l \in T} ||\nabla \mathcal{L}(w^-_{Foci_l}, D') ||_2 + \\
    & \sum_{l \in R} ||\nabla \mathcal{L}(w^*_{l}, D') ||_2
\end{align}
Based on the above argument of minimal propagation, the second term above goes to $0$ for layers/slices in $R$.
Therefore,
\begin{align}
    \label{eq:foci_res}
    ||\nabla \mathcal{L}(w^-_{Foci},D')||_2 = \sum_{l \in T} ||\nabla \mathcal{L}(w^-_{l}, D') ||_2 
\end{align}
and as such the gap between this and the full update is only the difference on the set $R$, shrinking as $O(1/n^2)$.
% \begin{align}
%     \label{eq:full_res}
%     ||\nabla \mathcal{L}(w^-_{Full},D')||_2 = \sum_{l \in L} ||\nabla \mathcal{L}(w^-_{l}, D') ||_2 
% \end{align}
% Since, FOCI selects sufficient sets, we have $T \subset L$. Hence, 
% \begin{align}
%     ||\nabla \mathcal{L}(w^-_{Foci},D')||_2 < ||\nabla \mathcal{L}(w^-_{Full},D')||_2 
% \end{align} 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:fociconv}}
% Sampling with weights proportional to the Lipschitz constant of individual filters/layers is an established approach in optimization \cite{gorbunov2020unified}. We argue that L-CODEC computes an approximation to optimal sampling probabilities for updating purposes. Under a mild assumption that the sampling probabilities have \emph{full} support, it turns out that correctness of our approximate (layer/filter selection) procedure can be guaranteed for unlearning purposes using recently developed optimization tools \cite{gower2019sgd}. By adapting results from \cite{gorbunov2020unified}, we can show the following theorem that summarizes the main result of our slice-based unlearning procedure.
\begin{theorem}
Assume that layer-wise sampling probabilities are nonzero. Given (user specified) unlearning parameters $\epsilon,\delta$, the unlearning procedure in Algorithm~\ref{alg:blockunlearn} is $(\epsilon',\delta')-$forgetting where $\epsilon'>\epsilon,\delta'>\delta$ represent an arbitrary  precision (hyperparameter) required for unlearning. Moreover, iteratively applying our algorithm converges exponentially fast (in expectation) with respect to the precision gap, that is, takes (at most) $O(\log\frac{1}{\mathbf{g_{\epsilon}}}\log\frac{1}{\mathbf{g_{\delta}}})$ iterations to output such a solution where  $\mathbf{g_{\epsilon}} = \epsilon'-\epsilon>0,\mathbf{g_{\delta}}=\delta'-\delta>0$ are gap parameters.
\end{theorem}
\begin{proof}
Our proof strategy is to show that our update step in Algorithm 1 is a specific form of Randomized Block Coordinate Descent (R-BCD) method. Then, we simply apply existing convergence rates of RBCD for general smooth minimization problems.  In particular, our method can be seen as an extension of SEGA method in Corallary A.7. \cite{gorbunov2020unified} where the descent direction is provided by using inexact inverse hessian metric \cite{loizou2020convergence}.  The key difference in our setup is that the sampling probabilities are computed using the CODEC procedure instead of the random sampling at each step. We make the following three observation in our setup  that immediately asserts correctness of the procedure. 

First, by our construction in equation~\eqref{eq:perturbsupp} in the main thesis, the sampling probabilities have full support. That is, the probability of selecting a particular weight in the neural network is strictly positive since $\xi\sim\mathcal{N}(0,\sigma^2), \sigma>0$ is a continuous distribution which has unbounded support. Second, the overall rate of speed of convergence depends on the condition number of the (fixed) Hessian at the optimal solution since exact $(\epsilon,\delta)$ unlearning is equivalent to linear least squares problem. Third, our update step is equivalent to a projected (or sketched) primal step, see equation 13 in (ArXiv Version \cite{loizou2019convergenceArxiv}). From these observations, we can see that our overall method is equivalent to SEGA in \cite{gorbunov2020unified} or its noisy extension since we use only a small set of samples (to be unlearned) at each iteration. Consequently, we obtain the deterministic geometric rate of convergence (in expectation)  by applying Corollary A.8. where $\sigma$ in their work corresponds to the $\epsilon'-\epsilon>0$ gap in our setup. Now, to get the probabilistic $\epsilon',\delta'$ unlearning guarantee for the solution presented by our algorithm, we use Lemma 10 in \cite{sekhari2021remember} on the solution returned, completing our proof.
\end{proof}
% Our result is  slightly different from Nesterov's acceleration since we do not use previous iterates in a momentum or ODE like fashion. The acceleration that we obtain here is closer to primal-dual algorithms where knowing nonzero coordinates at the dual optimal solution   (better duality gap)  can simply be used to accelerate primal convergence \cite{diakonikolas2019approximate}. Moreover, since our approach is {\em randomized}, the dynamics can be better modeled using the SDE framework for unlearning purposes \cite{simsekli2020fractional}. 
% In our implementation, we do not  compute anything extra, although remains feasible for future extensions.
% And relate to additive error on $(\epsilon,\delta)$ forgetting in the beginning...