\chapter{Efficient Learning and Unlearning via Large-Scale Conditional Independence Testing} \label{chap:lcodec} 
\todo{ch5 notation pass, remove redundant bknd}

While selecting a more efficient model \textit{a priori}
can be helpful when we can plan ahead of training,
it may sometimes be the case
that a task may require selection
after a model has been trained.
In this chapter we will 
address this post-hoc selection task
from a parameter-focused point of view,
motivated by the need for an efficient way of 
``scrubbing'' existing models of samples
they have been trained on.
At this time, unlearning in this context has
only been analyzed under the assumption
that model updates can be made over the \textit{entire} model, but the forms of provable updates have limited
practical deployments for feasibility
as we will see.
Using conditional independence ideas
we will identify a parameter subset sufficient for
efficient unlearning.
Work in this chapter was first published
in the conference on Computer Vision and Pattern Recognition~\citep{lcodecunlearn}.

\input{5_unlearn/inputs/_intro}
\input{5_unlearn/inputs/_setup}
\input{5_unlearn/inputs/_related}
\input{5_unlearn/inputs/_mbs}
\input{5_unlearn/inputs/_perturb}
\input{5_unlearn/inputs/_sel_unlearn}
\input{5_unlearn/inputs/_impl}
\input{5_unlearn/inputs/_exps}
\input{5_unlearn/inputs/_conclusion}