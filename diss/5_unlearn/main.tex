\chapter{Efficient Learning and Unlearning via Large-Scale Conditional Independence Testing} \label{chap:lcodec} 

While selecting a more efficient model \textit{a priori}
can be helpful when we can plan ahead of training,
it may sometimes be the case
that a task may require selection
after a model has been trained.
In this chapter we will 
address this post-hoc selection task
from a model \textit{parameter} point of view,
contrasting our 
approach from Chapter~\ref{chap:ott}
where we inform a reduced-parameter
\textit{model architecture}
before training.
Here we are motivated by the need for an efficient way of 
``scrubbing'' \textit{existing }models,
to address requests for sample deletion.
Unlearning in this context has
only been analyzed under the assumption
that model updates can be made over the \textit{entire} model,
but the forms of provable updates have limited
practical deployments for feasibility
as we will see.
Using conditional independence ideas
we will identify a \textbf{parameter subset} sufficient for
efficient unlearning.
Work in this chapter was first published
in the conference on Computer Vision and Pattern Recognition~\citep{lcodecunlearn}.

\input{5_unlearn/inputs/_intro}
\input{5_unlearn/inputs/_setup}
\input{5_unlearn/inputs/_related}
\input{5_unlearn/inputs/_mbs}
\input{5_unlearn/inputs/_perturb}
\input{5_unlearn/inputs/_sel_unlearn}
\input{5_unlearn/inputs/_impl}
\input{5_unlearn/inputs/_exps}
\input{5_unlearn/inputs/_conclusion}