\chapter{Introduction} \label{chap:intro} 

Modern applications of machine learning have become ubiquitous.
Most interactions with daily technologies now intrinsically involve 
a request to some ``smart`` system in the ``cloud'', 
where those interactions range from
a request for map directions 
to simply loading a webpage.
Neural network models, and the recent advances of deep learning,
have enabled these systems that 
make such applications possible.
These models have achieved
human-level performance on learning tasks
including image classification~\citep{resnet,alexnet}, image segmentation~\citep{segmentation}, video analysis~\citep{zhang2016video}, text understanding and generation~\citep{bert,gpt}, and have slowly begun to solve more fundamental scientific problems such as protein folding~\citep{protein} drug discovery~\citep{drugdisc}, and medical diagnosis~\citep{diag}.
While this performance is largely attributed to model size,
the abundance of high quality training data
has equally contributed to real world performance,
enabling model training over millions of real world samples~\citep{imagenet,laion},
and potentially billions of synthetic samples via environment simulation~\citep{mcts}.

While deployment in some domains (recommender systems, object detection) may benefit almost unconditionally from this vastly expanded capability, rightful hesitancy has limited their widespread use in particular applications where impacts on individuals, people, or environments may be at stake.
These ``last mile'' concerns take a few forms.
% Because a completely accurate model is still out of reach, an important question that needs to be answered is: which inputs or individuals are being given incorrect outcomes, and why?
% maybe a sentence suggesting unlearning/removal
In mission critical applications such as medical diagnosis, 
the impact of an error can be extremely large,
even if a misprediction happens extremely infrequently.
Additionally, large scale model training and architecture search
can require exorbitant amounts of energy producing high emissions,
and their scale can limit market participants
to only large actors with vast existing resources.
The accessibility and effectiveness of these models can also vary significantly based on the training data, and disparate outcomes can be exacerbated by existing social inequity.

While existing human or ``natural'' systems that these models aim to assist are not perfect, 
our real world has developed norms and regulations that 
enable them to function.
A medical diagnosis might require a physician to explain what symptoms led them to that particular conclusion.
Energy metering and carbon taxes may be applied to limit
emissions.
Regulatory satisfaction may require 
analysis proving equal opportunity,
or that specific protected classes
are not used in decision making.
% Specifically, these can include ideas as simple as the Hippocratic Oath and medical malpractice insurance, to asking your doctor what symptoms lead them to a particular diagnosis.
These ideas are difficult to directly translate to automated machine learning systems,
but proxies have been identified that we can build upon.

\begin{figure}
    \centering
    \includegraphics[trim={0 3cm 0 3cm},clip,width=0.95\textwidth]{diss/1_intro/figs/dl.png}
    \caption[Modern machine learning pipelines]{Machine learning training and inference visualization.}
    \label{fig:dl}
\end{figure}
These norms and regulations answer a number of questions we may also try to pose to our machine learning models.
What is the cost to learn this task?
What led to this particular outcome?
Why is this outcome different from another?
% We will explore how these questions can be formulated concretely. 

If the answer to these questions is negative or unknown,  follow-up questions all take an interesting form:
Can we learn a smaller model with similar performance? 
Can we identify the most important features? 
Which individuals or groups are being treated unfairly, and can we change that?
These questions ask us to identify a \textit{subset} of some relevant set, and this identification is our focus here.

% Moving specifically to machine learning methods,
Taking a step back, let's take a look at a representative system. Figure~\ref{fig:dl} illustrates a typical learning pipeline. 
A dataset is collected and used to train a model, by minimizing the error over
those samples in the dataset (top).
A ``sample'' can be a single measured value, or it can
be a large, highly structured object with many ``features.''
The model is made up of some ``parameters'' that are 
tuned during training to learn a good predictor over the training dataset.
This model is then used to predict, or \textit{infer}, on new
data seen ``in the wild'' (bottom).
Our questions above are formally asking to identify subsets of these objects: is a subset of the model parameters sufficient for learning? Which subset of the features are important for a prediction? Which subset of the dataset exhibit a specific attribute?

% \textit{Explainability} can be seen as identifying important features of the input, as well as parts of the model (parameters) that ``light up`` for that input. \textit{Fairness} can be evaluated via measures over subsets of the data that correspond to specific groups. 

\begin{mdframed}[style=MyFrame]
\em 
\textbf{This thesis} focuses its main efforts on identifying these important subsets of model, feature, and sample space, to enable answering questions necessary for mainstream adoption of machine learning methods.
\end{mdframed}

% In this dissertation, we explore the sizes of these models, samples, and datasets, and 
% analyze under what situations 
% a smaller \textit{subset} of them may be sufficient or important
% for questions that run parallel to standard performance and accuracy measures.

Let us step a bit deeper into a basic illustrative example. In order to ease understanding, we can first begin with a basic formulation of learning methods, from which the questions above can take specific forms. 
Learning methods typically  try to identify a function mapping (model) that is able to effectively complete a specific task.
% In Figure~\ref{fig:dl}, a model is trained using examples of classification task (top), in order to accurately predict the class of a newly provided input (bottom).
% \begin{figure}
%     \centering
%     \includegraphics[trim={0 3cm 0 3cm},clip,width=0.95\textwidth]{diss/1_intro/figs/dl.png}
%     \caption[Modern machine learning pipelines]{Machine learning training and inference visualization.}
%     \label{fig:dl}
% \end{figure}
Say we have some dataset comprising of sample pairs $(x,y)$, where we wish to predict $y$ from $x$.
Our prediction, say $\hat{y}$, might be the output of some unknown function $f$ that we attempt to learn. 
Let our approximation to this function be $\hat{y}:=\hat{f}(x)$.
This can take many forms, 
based on assumptions and prior information we may have on the relationships among the data. 
Consider the simple \textit{linear} case,
where we want to learn some parameter $w$ such that $y = w\cdot x$. 
Given $n$ sample pairs $(x_i,y_i)$ indexed by $i$, traditional statistics and optimization literature yield the following \textit{least squares} problem formulation, where we want to minimize the ``squared error'' between the observed values $y_i$ and the predicted $\hat{y}_i:= w\cdot x_i$:
\begin{align}\label{eq:lq}
\hat{f}:=\hat{w} = \mathop{\arg\min}_{w} \sum_{i=1}^n (y_i - w\cdot x_i)^2
\end{align}
This formulation expands without much change to a multi-dimensional form of the input $x$ and respectively, $w$: the canonical case where a number of features, or \textit{covariates} (e.g., symptoms), are used together to predict the outcome (e.g., diagnosis). 
If we are interested in which features of $x$ are important, we can look at the relative values of the learned ``weights'' $w$. In this simple setting, the importance of a feature (say $x^j$) can be exactly determined by the importance of the parameter ($w^j$).
A weight value far from zero may indicate that corresponding feature is important for diagnosis.
% If instead we are interested in which samples are most important, we can use existing methods for sample reweighting or methods that use standard assumptions to efficiently identify important subsets.

In this case and others, traditional statistical learning methods 
have been studied 
for many decades.
Linear regressors, decision trees, and support vector machines
have all been analyzed under these lenses.
% ,
% and as the modern machine learning community
% has returned to these questions recently,
% so has a renewed interest in their methods of analysis.
New research focuses
particularly on the differences
associated with moving from classical \textit{under-parametrized} models to
modern (deep) \textbf{over-parameterized} models: where
the model size vastly outnumbers the number
of input samples.
% , and may even be comparable to 
% the \textit{entire sample space.}
Methods for estimating the number of samples needed,
the time to learn a particular task,
and the generalization ability 
all require new perspectives in this regime.
While nascent, this research
attempts to fill the gap between
statistical and deep models to enable similar measures of sample influence, feature importance, and model understanding. 

\paragraph{A full picture.}
Let us expand our notation from the example above to consider this more general framing.
Consider a dataset $X:=\{x_i\}_{i=1}^n$ of size $n$ where each data point $x_i$ in the set $X$ is drawn from some underlying distribution over the domain $x_i \sim \cX^d$, 
with domain dimensionality (number of features) $d$.
A model $f$ is fit using a parametrization $\theta \in \Theta$,
with $\Theta$ the space of possible parametrizations (models) with some intrinsic dimension $p$. 
%While all three of these problems are closely related, they require different approaches. 
Generalizing the least squares``error measure'' from Eq.~\ref{eq:lq} to an arbitrary \textit{loss} $l$, we have
\begin{align}\label{eq:learning}
    \hat{f}:=\hat{\theta} = \mathop{\arg\min}_{\theta\in\Theta} \sum_{x \in X} l(f_\theta(x_i))
\end{align}
From an analysis perspective, 
we might be interested in any one of 
(a) subsets of input features $\cC \subseteq \cX$ that are important for the downstream task,
(b) associating model subsets $\cP \subseteq \Theta$ with specific inputs or groups of inputs, or 
(c) subsets or subgroups of samples $S \subseteq \{X\}^n$ that are sufficient or representative of the entire dataset.

Crucially, an uninformed search for a subset is computationally infeasible. For a superset of size $|S|$, The set of all subsets is the power set, with a size of $2^{|S|}$! If an identification procedure requires looking over all of these and choosing a ``best'' one by some metric, the procedure will be limited to very small supersets.
Efficient methods have been developed in each of the three contexts above to avoid this exponential search.

\begin{figure}
    \centering
    \includegraphics[trim={0 3cm 0 3cm},clip,width=0.9\textwidth]{diss/1_intro/figs/feat_select.png}
    \caption[Visualization of feature selection]{An example of identifying specific features important to the learning task.}
    \label{fig:feat_select}
\end{figure}
\paragraph{Feature Selection.} 
With more complex models $f$ compared to the linear case above, newer ``black-box'' methods have been developed for identifying important features. From the more pure statistics side, scan statistics~\citep{scanstat,scanstatlrt} allow for a structured ``scanning'' over the input space, skipping subsets unlikely to provide additional information for the measure of interest.
Further on the deep learning side, adaptations of sensitivity analysis, via noise addition and perturbations have found success~\citep{yeung2010sensitivity,zhang2015sensitivity}, alongside activation mapping~\citep{cam,selvaraju2017grad}.
These methods typically generate an analagous ``weighting'' over the input space, identifying features most salient for the specific task (Figure~\ref{fig:feat_select}).

\begin{figure}
    \centering
    \includegraphics[trim={0 3cm 0 3cm},clip,width=0.9\textwidth]{diss/1_intro/figs/param_select.png}
    \caption[Visualization of parameter selection]{An example of identifying specific parameters important to the learning task.}
    \label{fig:param_select}
\end{figure}
\paragraph{Parameter Selection.} 
Selection in the model space generally takes two forms. First, as a prior, restriction, or assumption over the model space, and second, as a post-hoc method for an ``explainable'' proxy.
Regularization, sparsity, and gating methods are often used independent of the type or size of the model, to encourage the solution to fall within a specific region of the model space.
% In non-deep settings these methods come with strong theoretical guarantees. 
The theoretical underpinnings of these methods in deep learning are still being actively researched~\citep{hardt2016train,jacot2018neural,neyshabur2014search}, but the methods have been nonetheless been effective in practice. 
On the post-hoc side, of particular interest is the parameters relevant to specific regions of the input space \textit{after} training (Figure~\ref{fig:param_select}). Here, recent analysis of deployed networks has shown this to be true~\citep{bau2017network,fong2018net2vec}, and current work continues to explore these network regions to aid in interpretability and explainability.

\begin{figure}
    \centering
    \includegraphics[trim={0 4.5cm 0 4cm},clip,width=0.9\textwidth]{diss/1_intro/figs/sample_select.png}
    \caption[Visualization of sample selection]{An example of identifying specific samples important to the learning task.}
    \label{fig:sample_select}
\end{figure}
\paragraph{Sample Selection.} Many methods have been developed for outlier detection within training or testing sets~\citep{huang2020feature,ren2019likelihood} \textit{after} training. ``In-the-loop'' methods for accounting for ``outlierness'' behave similarly to accounting for group or individual fairness while training~\cite{mehrabi2021survey}. Unfortunately, once samples are identified in some manner, post-hoc adjustments to a trained model are generally very difficult. Recent work has focused on ``unlearning'', or removing a sample's influence on a model without retraining. If specific samples can be uniquely identified, performance and privacy reasons may require these specific interventions to reduce the ``influence'' of that sample subset.

\begin{mdframed}[style=MyFrame]
\textbf{ Thesis Goal: }
\em Identify, construct, and evaluate methods for \textbf{efficient} subset identification in modern machine learning feature, model, and input spaces.
\end{mdframed}

\section{A Few Motivating Examples}
Consider a traditional machine learning classification task in which we would like to predict whether an individual has a specific disease condition based on a medical resonance image (MRI) scan of their brain. Our input feature $x$ may consist of a 3D-array of values lying in $\RR^{x\times y\times z}$ measuring some intensity of the imaging modality at each voxel, indexed by a tuple $(i,j,k)$.
Our outcome variable $y$ may simply be a binary label of whether the input scan has been labeled by a radiologist as one demonstrating typical disease characteristics.
Using an off the shelf 3D convolutional neural network with adjustments to match our input size, we can very quickly set up and train a system to predict disease presence with a high degree of accuracy.

\begin{example}
    feat select, region showing diff, methods work ,but hard for longitudinal ,theoretical guarantees.
\end{example}
\begin{example}
    model select
    evaluate generalizability
    if samples have different activations, maybe overfitting,
    even so, iding these activations may not be sufficient, dependencies among network regions may cause correlations making it difficult to find the true sufficient set
\end{example}
\begin{example}
    feat select, region showing diff, methods work ,but hard for longitudinal ,theoretical guarantees.
\end{example}

% What features are most important for prediction?
% Which samples were most important for my training?
% Can we understand when a model is certain or uncertain about its output?
% Are there layers in my network that have learned a particular subtask?
% Questions of robustness, bias, influence, fairness, and importance have become central questions to contemporary machine learning research \citep{doshi2017towards,mehrabi2021survey,amodei2016concrete}.
% machine learning, etc.

% Feature selection in the case of
% typical regression or classification 
% takes some form of learning parameters $\theta$ that allow for $\hat{y} = f_\theta(x)$ to be close to the true outcome of interest $y$.
% While forms of data $X := (x,y)$ may simply be continuous and real-valued, modern machine learning has greatly expanded formulations of the classical learning problem to include a wide variety of structured learning problems~\citep{nowozin2011structured}. 
% Consider the case when a high-dimensional input is used to predict an output with a highly-parametrized model. 
% Once learned, obvious questions arise as discussed above: are there specific low-dimensional spaces in either the input or the model space that are most important or necessary for the global learning problem of interest? Are there specific subspaces associated with particular subproblems of the global problem?
% The machine learning literature has come up with a number of ways to identify analogs of these spaces, 
% including extensions of sensitivity analysis to deep learning~\citep{yeung2010sensitivity,zhang2015sensitivity}, and constructing and identifying nonzero model subsets via particular model choices such as activations~\citep{selvaraju2017grad} and regularizers.
% In classical settings these are well understood: decision trees naturally provide ease of interpretibility via the information used to choose splits, and both linear and kernel support vector machines have been analyzed to provide for measures of sample importance via distances to the margin as well as feature importance via weights defining the learned hyperplane~\citep{Mitchell97}.
% Attention and saliency maps have emerged as popular new methods,
% given their ease of implementation and interpretation~\citep{sutskever2014sequence,vaswani2017attention,selvaraju2017grad}.
% By learning dimensions of a given input that are particularly important, either in a hard (binary) or soft (continuous weighting) manner, model builders are better able to understand and interpret what a model has learnt.
% The specific ideas of attention notwithstanding, many of these existing methods are far removed from traditional hypothesis testing frameworks.
% While some work has begun in this direction~\citep{tansey2018black},
% there remains a gap in direct identification of subsets and structures in these spaces that can be defined in statistically rigorous manners.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{example-image-a}
    \caption[A simple subset selection example]{\color{red} Identifying and selecting in MRIs, subset, sample, model ID.}
\end{figure}

% \paragraph{A specific example.} 

------------------------------------------

below here will be moved and arranged with the "selection" sections and here if relevant

------------------------------------------

While attention can be directly applied to the network in order to identify ``hotspots" in the input space relating to the learned classification task, 
given the high-dimensional nature of the input
and the relatively small sample size 
associated with medical imaging data, 
it is very likely that an area of interest identified
may be an intricacy of the training samples used rather than truly a region of disease signal.
Class activation maps (CAMs) may be unclear, and can often associate with image artifacts unrelated to the scientific task~\citep{adebayo2018sanity}.
Methods of generalization may help to increase confidence in identified regions, but statistical guarantees often remain out of reach.

Furthermore, most recent problems associated with medical data have moved past simple difference detection: trends over time, and the ability to predict {\em future} disease development has by far become the setting of most interest.
Given an image of a healthy individual, is it possible to predict what their scan, or their future disease diagnosis, may be up to 10, 20, or more years in the future?
If a number of scans have been collected over some timeframe, can the \textit{trajectory} of the individuals' development be extrapolated to estimate progression?
As traditional models extended for temporal analysis grow in both size and complexity,
a number of subproblems explicitly related to model and input subspaces arise. Here we address two such problems: \textbf{statistically rigorous identification of temporally evolving subsets}, and \textbf{characterizations of deep models that enable efficient training of recurrent models with large scale time-varying data}.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{example-image-b}
    \includegraphics[width=0.4\textwidth]{example-image-c}
    \caption[Selecting feature subsets]{\color{red} Blue/green figure from covtraj presentations, figure from OTT.}
\end{figure}

With the rapid growth of AI and machine learning applications has come valid concerns regarding both guarantees of privacy.
Recent technology legislation has made the importance clear in all aspects of data use,
and particular projects and groups have demonstrated that machine learning is not independent of
this need \citep{Exposing}.
A new issue raised within this intersection is the ``right to be forgotten".
If a model has been trained with a particular users' data, 
they should have some recourse or right
to both remove their data from the training set,
and also know that the model has not learned from their data.
On the surface, this poses a significant problem for model builders
and organizations that spend large amounts
of time and resources in 
training deep learning models.
As we will see, 
\textbf{identification of model parameter subsets}
that are particularly important
for a particular sample's influence
in a model enables \textit{efficient machine unlearning}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{example-image-b}
    \caption[Selecting parameter subsets]{\color{red} different paths for different samples figure from codec.}
\end{figure}

A sample's particular influence on model parameters aside, the identification of influential samples or subsets of samples more generally is of independent interest. 
Traditionally a rigorous area of study under classical statistics, outlier detection and accounting have become a subfocus for many within the machine learning community as well \citep{golatkar2020eternal,golatkar2020forgetting,huang2020feature,ren2019likelihood}.
While subgroups of input samples may be outliers, it is more often the case that they represent known heterogeneity within the data. 
These differences are typically marked using 
group information known a priori, and 
most learning tasks aim to learn tasks
in a \textit{subgroup-independent} manner.
Optimization and regularization methods with this focus come under the umbrella of model fairness, and instead of identifying and boosting independences within the model or data, we aim to minimize them.
However, many existing methods do not scale well as the number of subgroups grows, as is often the case when intersections of protected classes must be considered. In the sequel we identify and construct a particular solution for \textbf{groupwise fairness that enables efficient in the loop fairness regularization}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{example-image-b}
    \caption[Equating sample subsets]{\color{red} group disparity figure from demd pres.}
\end{figure}

\begin{mdframed}[style=MyFrame]
\em 
Here we focus our effort on identifying these important subsets of model, feature, and sample space for feature association, model size reduction, model unlearning, and, fairness. Specifically, taking advantage of both existing statistical and geometric methods, we develop new methods for localizing subsets in a range of settings from hypothesis testing to deep learning.
\end{mdframed}

\section{Thesis Scope and Contributions}

We explore the intersections of classical statistical and geometric constructions with modern machine learning methods. 
Figure~\ref{fig:scope} shows the overall scope projected along three axes: feature, parameter, and sample spaces.
Below we briefly introduce the main problems studied in this thesis.
\begin{figure}[!ht]
    \centering
    % \includegraphics[width=0.99\linewidth]{scope.pdf}
    \includegraphics[width=0.95\linewidth]{diss/1_intro/figs/thesis_scope.png}
    \caption[Thesis Scope]{Thesis scope, projected over three representative axes. {\color{red} update chapter numbers shift by 1}}
    \label{fig:scope}
\end{figure}

\subsection{Second-Order Modeling and Group Difference Analysis over Time}

Recent results in coupled or temporal graphical models offer schemes for estimating the relationship structure 
between features when the data come from
related (but distinct) longitudinal sources. A novel application of these ideas is for analyzing group-level differences, i.e., in identifying if {\em trends} of estimated objects (e.g., 
covariance or precision matrices) are different across disparate conditions (e.g., gender or disease). Often, poor effect sizes make detecting the \textit{differential} signal 
over the {\em full} set of features difficult: for example, 
dependencies between only a {\em subset of features} may manifest differently across groups.
We first suggest
a parametric model 
for estimating trends in the space of $\SPD$ matrices as a function of one or more covariates.
We will then generalize scan statistics to graph structures, 
to search over distinct subsets of features (graph partitions) whose temporal dependency structure may show statistically 
significant group-wise differences.
We will theoretically analyze the Family Wise Error Rate (FWER) and bounds on Type 1 and Type 2 error. 
On a cohort of individuals with risk factors for Alzheimer's disease (but otherwise cognitively healthy), 
we 
find scientifically interesting 
group differences where the default analysis, 
i.e., models estimated on the {\em full graph}, do not survive reasonable 
significance thresholds. 
% Preliminary work on this was published in \citep{covtraj}.


\subsection{Efficient Tensor Representations for Feasible Temporal Deep Learning}

Modern deep networks have proven to be very effective for analyzing real world images.
However, their application in medical imaging is still in its early stages,
primarily due to the large dimension of three-dimensional images, requiring enormous convolutional or fully connected layers --
if we treat an image (and not image patches) as a sample. 
These issues only compound when the focus moves towards longitudinal analysis
through recurrent structures, and when a point estimate of model parameters is insufficient 
in scientific applications where a reliability measure is necessary.
Using insights from differential geometry, 
we will adapt 
the tensor train decomposition to construct networks
with significantly fewer parameters,
allowing us to train powerful recurrent networks on whole brain image volumes. 
We analyze 
the \textit{orthogonal tensor train},
and demonstrate its ability to express a standard network layer both theoretically and empirically.
We 
demonstrate its ability to 
effectively reconstruct whole brain volumes
with faster convergence and stronger confidence intervals
compared to the standard tensor train decomposition. 
We provide code and show experiments on the ADNI dataset
using image sequences to regress on a cognition related outcome.
% Preliminary work on this was published in \citep{ott}.

\subsection{Practical Unlearning via Large-Scale Conditional Independence Testing}

%With AI systems extensively using personal %data for model training, 
Recent legislation has
led to interest in {\em machine unlearning}, i.e., removing specific training samples from a {\em predictive} model as if they never existed in the training dataset. 
Unlearning may also be required due to  corrupted/adversarial data or simply a user's updated privacy requirement.
For models which require no training ($k$-NN), 
simply deleting the closest original sample can be effective. 
%However, it is not clear how such approaches can be used to unlearn 
%models that contain rich information learned from the original data.
But this idea is inapplicable to models which learn richer 
representations.
%from data. 
%Recently, optimization-based unlearning estimators have been proposed, but 5their 
Recent ideas leveraging optimization-based updates
scale poorly with the model dimension $d$,  
due to 
inverting the Hessian of the loss function. %with an overall cost of $O(d^3)$ 
%is prohibitive.
We describe
a variant of a new conditional independence coefficient, 
L-CODEC, to identify a subset of the model parameters with the most semantic overlap on an individual sample level. 
Our approach completely avoids the need to invert a (possibly) huge matrix. 
By utilizing a Markov blanket selection, 
we find
that L-CODEC is also suitable for deep unlearning,
as well as other applications in vision.
Compared to alternatives, L-CODEC makes approximate unlearning possible 
in settings that would otherwise be infeasible, 
including vision models used for face recognition, 
person re-identification 
and NLP models that may require unlearning samples identified for exclusion.
% Preliminary work on this will appear in \citep{lcodec}.


\subsection{Reducing Subgroup Fairness via High Dimensional Earth Mover's Distances}

Optimal transport has recently emerged as a useful tool for machine learning through its connections with geometry, statistical machine learning, and through practical algorithms. Existing methods that leverage optimal transport often  regularize using  a Wasserstein metric or by computing barycenters, for example. %which are effective when distributions are continuous and known, or when measures of interest are discrete.
% Our formulation allows for a discretization of continuous measures that drop in directly to classical  formulations of the Earth Mover's Distance. 
We leverage optimal transport, except that we take advantage of a recently-introduced algorithm that computes a generalized earth mover's distance.
Not only is this algorithm computationally cheaper to compute compared to existing barycentric measures, but our method has the additional  advantage that gradients used for backpropagation can be directly read off of the forward pass computation, which leads to substantially faster model training.
We provide technical details about this new regularization term and its properties, 
and 
experimental demonstrations of improved training speed over existing Wasserstein-style methods.

{\color{red}
\subsection{Applying Conditional Independence Testing for better Understanding of Preclinical Alzheimer's Disease}

The final chapter of this thesis applies some of the tools developed above in analyzing preclinical Alzheimer's disease patients. 
In these studies, 
we aim to identify conditionally independent features and subjects that are particularly important to the prediction and estimation of
key disease outcomes,
as a function of a number 
of demographic, neuropsychological,
genetic,
and imaging data collected as 
part of an ongoing consortium 
to understand the progression
of Alzheimer's disease in younger, 
asymptomatic populations.
In what follows we present
exploratory analysis
on a small, easily 
digestible subset of the available data,
that lays the foundation for
further analysis.
}
% This work is the most forward looking, and aims to be a stepping stone toward a rigorous 

\section{Outline}
Chapter 2 covers the essential background necessary for the developments presented in the following chapters, including specifics of graphs and hypothesis testing, as well as relevant modern methods for learning and optimization.
In Chapters 3 through 7, we describe four perspectives to address subset identification.
Chapter 3 explores and focuses on the identification of feature subsets varying over time.
In Chapter 4 we describe a method of constraining the parameter space in a particular manner
that enables more efficient large scale neural networks.
Next, Chapter 5 provides a solution to the machine unlearning problem,
enabled through a particular conditional independence parameter selection scheme, vastly reducing network update costs.
Chapter 6 ends with a unique solution to subgroup fairness, 
where we take advantage of an efficient solution to
the $d$-dimensional earth mover's problem
to regularize large models when the number of subgroups can be large.
{\color{red} Chapter 7 describes future work, focused on applying a particular solution from Chapter 5 to understanding relationships among
disease indicators and biomarkers associated with developing Alzheimer's Disease.}


%%%%%%%%%%%%% 
% Some old stuff

% Significant progress in the modern development of machine learning has
% been built upon connections and patterns identified across myriad
% interdisciplinary fields of study.
% Up through the mid 2000's, 
% many of these methods were inspired by and interested in 
% highly focused and constrained problems. 
% With a reasonably sized input domain, could a model of roughly equal size be used to
% predict some output?
% Linear regressors, decision trees, and support vector machines were all answers to these questions, with their own
% varying degrees of scaling and complexity.
% These methods necessitated carefully constructed formulations with specific restrictions to the learnable function class,
% enabling straightforward analysis 
% for provable performance guarantees 
% and easy identification of critical training samples and important input features.

% Contemporary machine learning, however, has a vastly different modus operandi. 
% Driven in large part by the exponential growth of available computation via Moore's Law, \textit{deep learning} has fallen squarely in the realm of \textbf{over-parameterized} models.
% With these overparametrizations and computation capacity, the typical learning questions posed as maximizing accuracy or reducing error have largely been addressed for even large scale problems.
% As such, complementary questions have led to subfields focusing on other performance measures, such as robustness, fairness, interpretability, and explainability.
% Many solutions to these questions end up looking back at answers found for the under- or non-parametrized settings.
% While nascent, these approaches 
% attempt to fill the gap between
% statistical and deep models to enable similar measures of sample influence, feature importance, and model analysis. 
% Most notable amongst these newer approaches is that of (Self)-Attention in Neural Networks \citep{sutskever2014sequence,vaswani2017attention}.
% Other proposals 
% end up looking back at the types of analysis typical of those more classical under-parametrized or nonparametrized methods.

% Not limited to previous developments in learning or computation theory, the arguably most valuable contributions toward the exponential reduction in model error can be attributed to influences and intuitions taken
% from biology, psychology, neuroscience, and even XXX \citep{srivastava, etc}.
% Perhaps one explanation as to why this phenomenon exists may be attributed to the way in which deep learning evolved. 
% The classical learning goal of function approximation lends itself nicely to a system which allows for arbitrary complexity via simple changes (e.g., addition of neural network layers). % Foundational works building on the original neural networks particularly have taken advantage of constraining this space of functions to search over: 
% the most seminal case being those of convolutional filters for imaging data. 
% While ``constraints" of this form have helped tremendously in model performance on modern vision and language machine learning tasks (GANs, Recurrent Networks, Residual Layers, Transformers, etc.), the ability to identify \textit{subsets} of important samples, input features, and model parameters has lagged significantly behind the development of these methods.
% Recently larger interest has been taken by the community to understand and interpret models with this view, only after extremely large and opaque models have become ubiquitous.
%This lag directly explains the more recent interest in developing methods for understanding and interpreting large scale machine learning models.