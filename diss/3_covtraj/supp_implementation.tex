The workflow below describes one run of our model given a sparsity is specified for the oracle graph procedure.
\begin{enumerate}
\item \textbf{Oracle Graph.} As noted in the main paper, we use \textit{graphical lasso (glasso)} to generate an \textit{oracle graph}, which allows to define structured regions (subgraphs) for scan statistics on graphs. 
Each element of the input matrix $C$ in \eqref{eq:glasso} for glasso is generated by  calculating the slope for each position of the covariance matrix across the predictors for each group, and then taking the difference between the groups.
%The empirical \hjkchange{covariance} matrix used as input is generated by na\"ively calculating the slope for each position of the covariance matrix across the predictors for each group, and then taking the difference between the groups as the $C_{ij}$-th element. 
Equation \eqref{eq:glasso}, repeated here, is then solved using existing MATLAB interfaces to fast C implementations.
\begin{equation}
\Theta = \arg\min_{\Theta \succeq 0} \quad -\log|\Theta| + tr(C\Theta) + \lambda||\Theta||_{1}
\end{equation}

With sparsity parameter $\lambda$, this procedure generates a reasonably sparse \textit{oracle graph}.

\item \textbf{Candidate Subgraphs.} With the oracle graph in hand, we then construct the set of all ball subgraphs, as defined in Section \ref{sec:loc} of our main paper. By limiting ourselves to only a few ($D|V|$) subgraphs, we can perform scan statistics more efficiently.
%\begin{equation}
%\Rcal = \{ B(v,r) : v \in V,\ r \in \NN \}
%\end{equation}

\item \textbf{Characterizing the Null Distribution.} In the case where we have few samples, we cannot directly apply the $\chi^2$ result. In these cases, the null distribution is then characterized using permutation testing over all candidate subgraphs. For each subgraph the input data is permuted a number of times to generate a good representation of the distribution at that subgraph. All normalized (but not size-corrected) scan statistics are then calculated for all permutations across all subsets and then combined in order to create the null distribution.

\item \textbf{Calculating the Test Statistic} For a specific subset of the data, the scan statistic is calculated and corrected as described in Section \ref{sec:loc} of the main paper, over the original grouping of the data. For each group, the logitudinal-covariance GLM \eqref{eq:lcglm} is computed using the procedures in \S\ref{sec:effest}.
%%{\small\begin{align} \label{eq:manscanstat}
%%T^\ast=\max_{R\in\Rcal}\left(L_R-\sqrt{2\log{|\text{dim}(\Mc)|\over |\text{dim}(\Mc^R)|}}\right).
%%\end{align}}
%
\item \textbf{Region Identification.} We first identify all subsets whose statistic falls above the $\alpha$-level threshold specified. Then the subset-collection procedure outlined in the main paper, developed by \cite{jeng2010optimal}, is applied, and the non-overlapping critical regions are output.
\end{enumerate}
%

\subsection*{Numerical Considerations} 
%To fit this regression on the manifold, we require that each sample might live on the manifold. 
In practice, our empirical covariance matrices calculated on the sample data may not be positive definite. The matrix can be rank deficient when we do not have enough linearly independent samples. 
In addition, we may use a rank correlation matrix in its place, which also may not be PD.
To resolve this issue, we \textit{project} the empirical covariance matrix onto the symmetric-positive definite $\SPD(n)$ manifold. We first apply a standard procedure for transforming a 
symmetric matrix into a symmetric positive semidefinite (SPSD) one. As described in \cite{wu2005analysis}, the standard eigenvalue thresholding, or clipping, $\lambda_{SPSD} = \max(0,\lambda)$ is sensible 
since it provides the optimal projection of any matrix onto the SPSD manifold. 
Let $\Sigma = U\Lambda U^\top$ be the eigenvalue decomposition of the matrix $\Sigma$. The SPSD projection of $\Sigma$ is 
then $\text{proj}_{SPSD}(\Sigma) = U\text{diag}(\max(\lambda_1,0),\ldots,\max(\lambda_n,0))U^\top$. And so to project to the $\SPD(n)$ manifold we can simply add some epsilon to each element of the diagonal: 
%The final step, to ensure that the matrix is positive definite (PD), 
%requires adding a small mass, $\epsilon$, to each element of the diagonal. 
\begin{align}
\text{proj}_{SPD}(\Sigma) = U\text{diag}(\max(\lambda_1,0),\ldots,\max(\lambda_n,0))U^\top + \epsilon I
\end{align}
A remark on the term $\epsilon I$ will be useful here. We find that in experiments, numerical problems can arise if the smallest eigenvalue of the projected matrix is too small. 
By iteratively adding a 
small $\epsilon$ until the smallest eigenvalue is above our threshold, we ensure that the matrix is positive definite for the exponential and logarithmic maps. They are necessary for moving back and forth between the manifold and the tangent space.