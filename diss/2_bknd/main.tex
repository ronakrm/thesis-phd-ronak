\chapter{Background}\label{chap:bknd}

Here we will briefly describe some background concepts and ideas that will aid and facilitate discussion of subset identification in later chapters. 
Following some general notation,
we will begin with an overview
of classical probability and statistics,
and a focus on the hypothesis testing
schemes built upon in the third chapter.
This is followed by the basics of Riemannian differential geometry
and the geometry of tensor objects,
the key objects that enable the 
tests and efficiencies described in both 
Chapters 3 and 4.
Next we provide
an overview of optimization tools
and methods particularly suited
and developed for neural network methods,
alongside typical forms and objectives 
in the machine learning literature.
We conclude this chapter
with a discussion on optimal transport
methods in deep learning, important
for our discussion of novel optimization schemes
in Chapter 6.

\section{General Notations}
The following notations will be standard and followed consistently throughout the sequel, unless otherwise noted.
\begin{itemize}
\item Calligraphic capital letters will typically represent spaces, and capital letters will typically represent an instance or element from those spaces.

\item Set sizes will typically take the form of lowercase $n,d,p$, generally associated with the number of samples, the dimension of some feature space, and the number of parameters respectively. $i,j,k$ will be used for indexing various sets within context.

\item We use $\RR^d$ to represent $d$-dimensional vector space over the reals, and vectors will be lowercase typically as $x,y,z$ and matrices and higher order tensors will be denoted in uppercase, with local clarity on their distinction between a set instance.

\item $S$ will denote a of training examples.
A sample $s_i\in S$ may be indexed by $i \in [1,\ldots,n]$. 
For general and generative settings, $x_i$ may be used in place, and for predictive settings will be represented by the tuple $s_i:=(x_i,y_i)$.

\end{itemize}

\section{Probability and Independence}
\input{2_bknd/probstat.tex}

\section{Differential Geometry}\label{sec:diffgeom}
\input{2_bknd/dffgeom.tex}

\subsection{Tensors and their Geometry}
\input{2_bknd/tensors.tex}

\section{Deep Networks, Optimization, and Objectives}
\input{2_bknd/dnnopt.tex}

Each chapter in the sequel
is defined by a unique intersection
of the above ideas, leading to new insights
with respect to the subset selection problem of interest.
The relevant background will be referenced and refreshed 
as needed to avoid reader thrashing.\todo{lang., "thrashing" alternate?}