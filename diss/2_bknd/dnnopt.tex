The form of the function $f_\theta$ in~\eqref{eq:learning}
is critical in determining both
the types of optimization methods
that may identify a solution,
and the particular minimizer identified.
In the learning methods that follow,
$f$ will typically take the form of 
a deep neural network.
The advantages
and successes of deep neural networks
rely heavily on their ease of optimization:



Importantly,
the \textit{solution} identified can 
vary significantly based on the particular form 
of the function $f_\theta$.

\begin{verbatim}
networks
    objective
how to minimize?
    general optimization?
    gradients
    stochastic grad

NNs aside, lets go back to simple formulations where things are nicer...
linear programming
\end{verbatim}

\subsection{Losses and More Optimal Transport}
\begin{verbatim}
    OT can be framed as a linear program....
\end{verbatim}
    
