The form of the function $f_\theta$ in~\eqref{eq:learning}
is critical in determining both
the types of optimization methods
that may identify a solution,
and the particular minimizer identified.
In the learning methods that follow,
$f$ will typically take the form of 
a deep neural network.
The advantages
and successes of deep neural networks
rely heavily on their ease of optimization:
the \textit{computation graph} that 
underlies the neural network
allows for gradients
to be computed by parts
and accumulated via the chain rule.

Consider a simple function $f_\theta(x)$
that is defined as a linear combination of 
some parameters $\theta:=w, w\in \RR^d$ with $x\in \RR^d$ followed by 
a differentiable, nonlinear scalar \textit{activation} function $a(\cdot)$:
\begin{align}
	f_\theta(x) := a(w\cdot x)
\end{align}
If we have some estimate of the parameters $\theta:=w$,
then the gradient of the full network with respect to those parameters is
\begin{align}
	\frac{df}{d\theta} = \frac{df}{da}\frac{da}{dw}
\end{align}
where $df/da$ is the (known) derivative of the \textit{activation} function,
and $da/dw$ is exactly $x$, the derivative of a linear function.
With a direction of descent,
we can update the parameters via some update to minimize the functional $f$ of interest:
\begin{align}
	\theta_{t+1} = \theta_t + g(\theta_t,x)
\end{align}
where $g(\cdot)$ is some function of the full derivative $g(\cdot) := g(\nabla f_\theta(x))$,
and $\theta_t$ are the current parameter estimates.
Optimization proceeds and terminates when a certain amount
of iterations $t$ have completed,
or some stopping criterion has been reached,
typically that the gradient is small,
indicating that a minima has been identified.

\begin{verbatim}
full gradients not feasible, large samples,
hessians
\end{verbatim}



This formulation and gradient update generalize
to extremely large and complex stacks
of operations, and are what have enabled
the enormous success and ubiquity of learning
methods to this day.
Importantly,
the final minima identified can 
vary significantly based on the particular form 
of the function $f_\theta$, and,
in the case where the function $f$ is
not \textit{convex},
it can also depend on
the initial estimate $\theta_0$ at initialization.


forms of $f$, architectures
	architecture search
	initializations
		lottery ticket
		


\begin{verbatim}
networks
    objective
how to minimize?
    general optimization?
    gradients
    stochastic grad
\end{verbatim}

\subsection{Losses and Probability Measures}
loss functions, objectives
arbitrary distances
MSE
regularizers

probabilistic distances
KL
mutual information
MMD

\subsubsection{Optimal Transport}	
\begin{verbatim}
	OT definition
	earth movers bknd
	continuous
	discrete
	2d bknd
	when metric is abc,...
	nice props, existing work, more in chapter 6
    OT can be framed as a linear program....
\end{verbatim}
    
